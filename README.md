# Proyecto Bridge en PythonğŸ

Este proyecto utiliza un entorno virtual para evitar conflictos de dependencias entre paquetes.

## ğŸ§ª Crear entorno virtual

```bash
python -m venv myenv
```

## â–¶ï¸ Activar entorno

En Windows:

```bash
myenv\Scripts\activate
```

En macOS/Linux:

```bash
source myenv/bin/activate
```

## ğŸ“¦ Instalar dependencias

```bash
pip install -r requirements.txt
```

## ğŸ’¾ Guardar dependencias (al actualizar o agregar paquetes)

```bash
pip freeze > requirements.txt
```

## âŒ No olvidar

El entorno virtual (`myenv/`) no se sube al repositorio, ya estÃ¡ incluido en `.gitignore`.
Si corres varias veces el sistema, el JSON 'landmarks_data.json' se irÃ¡ llenando con mÃ¡s entradas (no sobreescribe).

## ğŸš€ Ejecutar backend local con FastAPI

Este proyecto expone un backend que permite recibir imÃ¡genes y procesarlas con MediaPipe para retornar los landmarks de las manos detectadas.

### ğŸ“Œ Comando para correr el backend local:

```bash
uvicorn app.api_server:app --reload
```

Esto abrirÃ¡ el servidor en:

```text
http://127.0.0.1:8000
```

Y la interfaz de prueba automÃ¡tica de la API (Swagger UI) estarÃ¡ disponible en:

```text
http://127.0.0.1:8000/docs
```

### ğŸ§ª CÃ³mo probar:

1. Abre Postman o entra a `/docs`.
2. Usa el endpoint `POST /detect`.
3. Sube una imagen que contenga una o mÃ¡s manos.
4. RecibirÃ¡s una respuesta JSON como:

```json
[
  {
    "handedness": "Right",
    "landmarks": [
      {"x": 0.1234, "y": -0.0456, "z": 0.0123},
      ...
    ],
    "label": "open_hand"
  }
]
```

### ğŸ“ Dataset generado

Cada vez que se detecta una mano, se guardan sus coordenadas 3D en el archivo:

```bash
data/landmarks_data.json
```

Este archivo crece con cada ejecuciÃ³n y puede usarse como base para entrenar modelos de clasificaciÃ³n de gestos mÃ¡s adelante.